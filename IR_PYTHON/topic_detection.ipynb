{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk\n",
    "from textProcessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\hp\\\\Desktop\\\\college-5-2\\\\IR\\\\lotte\\\\lifestyle\\\\dev\\\\collection.tsv'\n",
    "\n",
    "text = textProcessing.readDataset(path)\n",
    "text.columns=['text']\n",
    "text=text['text']\n",
    "text = textProcessing.normalization(text.head(500))\n",
    "text = textProcessing.toLowercase(text)\n",
    "text = textProcessing.delete_url(text)\n",
    "text = textProcessing.delete_punctuation(text)\n",
    "text = textProcessing.delete_dates_and_times(text)\n",
    "text =  textProcessing.delete_numbers(text)\n",
    "text = text.apply(textProcessing.tokenize_and_remove_stopwords)\n",
    "texts = text.apply(textProcessing.lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.039*\"dog\" + 0.009*\"dollar\" + 0.009*\"pet\" + 0.006*\"food\" + 0.006*\"one\"')\n",
      "(1, '0.016*\"dog\" + 0.012*\"cat\" + 0.010*\"dollar\" + 0.009*\"pet\" + 0.008*\"baby\"')\n",
      "(2, '0.025*\"dog\" + 0.013*\"cat\" + 0.010*\"dollar\" + 0.006*\"one\" + 0.005*\"pet\"')\n",
      "(3, '0.016*\"cat\" + 0.014*\"dog\" + 0.011*\"rabbit\" + 0.009*\"litter\" + 0.007*\"time\"')\n",
      "(4, '0.019*\"dog\" + 0.013*\"cat\" + 0.009*\"dollar\" + 0.007*\"like\" + 0.007*\"time\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a dictionary from the text data\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# create a corpus from the text data\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# train the LDA model on the corpus\n",
    "ldamodel = models.LdaModel(corpus, num_topics=5, id2word=dictionary)\n",
    "\n",
    "# extract the topics from the model\n",
    "topics = ldamodel.print_topics(num_topics=5, num_words=5)\n",
    "\n",
    "# print the topics\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 nWords: 0.097*\"cat\" + 0.022*\"may\" + 0.020*\"like\" + 0.016*\"help\" + 0.015*\"also\" + 0.015*\"old\" + 0.013*\"use\" + 0.013*\"something\" + 0.013*\"small\" + 0.012*\"toy\"\n",
      "n\n",
      "Topic: 1 nWords: 0.047*\"pet\" + 0.023*\"use\" + 0.023*\"water\" + 0.019*\"treat\" + 0.018*\"keep\" + 0.018*\"age\" + 0.017*\"like\" + 0.017*\"need\" + 0.016*\"behavior\" + 0.016*\"used\"\n",
      "n\n",
      "Topic: 2 nWords: 0.031*\"food\" + 0.024*\"time\" + 0.018*\"case\" + 0.016*\"keep\" + 0.016*\"away\" + 0.015*\"would\" + 0.014*\"need\" + 0.014*\"likely\" + 0.013*\"problem\" + 0.012*\"try\"\n",
      "n\n",
      "Topic: 3 nWords: 0.043*\"cat\" + 0.040*\"food\" + 0.031*\"fence\" + 0.021*\"would\" + 0.021*\"one\" + 0.019*\"even\" + 0.015*\"eat\" + 0.014*\"water\" + 0.014*\"way\" + 0.012*\"ive\"\n",
      "n\n",
      "Topic: 4 nWords: 0.035*\"pet\" + 0.024*\"child\" + 0.019*\"get\" + 0.018*\"one\" + 0.018*\"time\" + 0.016*\"like\" + 0.016*\"puppy\" + 0.014*\"take\" + 0.014*\"also\" + 0.013*\"dont\"\n",
      "n\n",
      "Topic: 5 nWords: 0.052*\"fish\" + 0.028*\"small\" + 0.026*\"volume\" + 0.020*\"start\" + 0.020*\"puppy\" + 0.020*\"noise\" + 0.019*\"need\" + 0.019*\"lot\" + 0.019*\"reward\" + 0.019*\"thing\"\n",
      "n\n",
      "Topic: 6 nWords: 0.025*\"month\" + 0.025*\"vet\" + 0.025*\"sound\" + 0.025*\"wood\" + 0.025*\"natural\" + 0.024*\"also\" + 0.023*\"water\" + 0.019*\"used\" + 0.019*\"time\" + 0.019*\"smell\"\n",
      "n\n",
      "Topic: 7 nWords: 0.102*\"rabbit\" + 0.099*\"litter\" + 0.051*\"box\" + 0.030*\"clean\" + 0.027*\"area\" + 0.024*\"use\" + 0.024*\"one\" + 0.016*\"go\" + 0.014*\"experience\" + 0.014*\"help\"\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    " num_topics = 8, \n",
    " id2word = dictionary, \n",
    " passes = 10,\n",
    " workers = 2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} nWords: {}\".format(idx, topic ))\n",
    "    print(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 374 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Print the most relevant topic\u001b[39;00m\n\u001b[0;32m     30\u001b[0m top_topic_index, top_topic_score \u001b[38;5;241m=\u001b[39m sorted_topics[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 31\u001b[0m top_topic \u001b[38;5;241m=\u001b[39m \u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_topic_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost Relevant Topic:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(top_topic)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\basemodel.py:19\u001b[0m, in \u001b[0;36mBaseTopicModel.print_topic\u001b[1;34m(self, topicno, topn)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_topic\u001b[39m(\u001b[38;5;28mself\u001b[39m, topicno, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a single topic as a formatted string.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m + \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (v, k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopicno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1218\u001b[0m, in \u001b[0;36mLdaModel.show_topic\u001b[1;34m(self, topicid, topn)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_topic\u001b[39m(\u001b[38;5;28mself\u001b[39m, topicid, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the representation for a single topic. Words here are the actual strings, in constrast to\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;124;03m    :meth:`~gensim.models.ldamodel.LdaModel.get_topic_terms` that represents words by their vocabulary ID.\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2word[\u001b[38;5;28mid\u001b[39m], value) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topic_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopicid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1249\u001b[0m, in \u001b[0;36mLdaModel.get_topic_terms\u001b[1;34m(self, topicid, topn)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, topicid, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the representation for a single topic. Words the integer IDs, in constrast to\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;124;03m    :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \n\u001b[0;32m   1248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1249\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopicid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1250\u001b[0m     topic \u001b[38;5;241m=\u001b[39m topic \u001b[38;5;241m/\u001b[39m topic\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# normalize to probability distribution\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     bestn \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(topic, topn, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 374 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "# Create a similarity index for the trained model\n",
    "index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    "\n",
    "# Convert the query to the LDA vector representation\n",
    "query_vector = dictionary.doc2bow(['give cat pill'])\n",
    "query_lda_vector = lda_model[query_vector]\n",
    "\n",
    "# Calculate the similarity between the query and each topic\n",
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    "sims = lda_index[query_lda_vector]\n",
    "\n",
    "\n",
    "# Sort the topics by similarity score (descending order)\n",
    "sorted_topics = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "# Print the most relevant topic\n",
    "top_topic_index, top_topic_score = sorted_topics[0]\n",
    "top_topic = lda_model.print_topic(top_topic_index)\n",
    "\n",
    "\n",
    "print(\"Most Relevant Topic:\")\n",
    "print(top_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs for Topic 0 :\n",
      "[2, 9, 28, 36, 38, 44, 46, 54, 67, 68, 74, 84, 86, 87, 89, 95, 101, 118, 143, 145, 148, 153, 183, 186, 189, 196, 198, 227, 230, 233, 234, 236, 261, 264, 281, 283, 285, 287, 288, 292, 302, 303, 305, 310, 327, 332, 368, 373, 375, 389, 400, 409, 415, 451, 454, 465, 470, 478]\n",
      "Similarity Scores:\n",
      "Topic 0 : 0.0\n",
      "Topic 1 : 0.0\n",
      "Topic 2 : 0.0\n",
      "Topic 3 : 0.0\n",
      "Topic 4 : 0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Topic ID to retrieve document IDs for\n",
    "topic_id = 0  # Replace with the desired topic ID\n",
    "\n",
    "# Retrieve document IDs for the given topic\n",
    "topic_docs = [doc_id for doc_id, doc_topic in enumerate(lda_model[corpus]) if max(doc_topic, key=lambda item: item[1])[0] == topic_id]\n",
    "\n",
    "# Print the document IDs for the topic\n",
    "print(\"Document IDs for Topic\", topic_id, \":\")\n",
    "print(topic_docs)\n",
    "\n",
    "\n",
    "# Preprocess the query\n",
    "query_tokens = ['give cat pill']\n",
    "\n",
    "# Convert the query to the binary vector representation\n",
    "query_vector = [1 if word in query_tokens else 0 for word in dictionary.token2id]\n",
    "\n",
    "# Calculate the Jaccard similarity between the query and each topic\n",
    "similarity_scores = []\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    topic_vector = lda_model.get_topic_terms(topic_id)\n",
    "    topic_terms = set([dictionary[id] for id, _ in topic_vector])\n",
    "    topic_vector = [1 if word in topic_terms else 0 for word in dictionary.token2id]\n",
    "    similarity_score = jaccard_score(query_vector, topic_vector)\n",
    "    similarity_scores.append(similarity_score)\n",
    "\n",
    "# Sort the topics by similarity score (descending order)\n",
    "sorted_topics = sorted(enumerate(similarity_scores), key=lambda item: -item[1])\n",
    "\n",
    "# Print the similarity scores\n",
    "print(\"Similarity Scores:\")\n",
    "for topic_id, similarity_score in sorted_topics:\n",
    "    print(\"Topic\", topic_id, \":\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.to_csv('C:\\\\Users\\\\hp\\\\Desktop\\\\lemmetized.csv', sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "# texts = pd.read_csv('C:\\\\Users\\\\hp\\\\Desktop\\\\lemmetized.csv' ,encoding='utf-8' ,sep='\\t')\n",
    "\n",
    "path = 'C:\\\\Users\\\\hp\\\\Desktop\\\\college-5-2\\\\IR\\\\lotte\\\\lifestyle\\\\dev\\\\collection.tsv'\n",
    "\n",
    "corpus = textProcessing.readDataset(path)\n",
    "corpus.columns =  ['text']\n",
    "corpus = corpus['text']\n",
    "\n",
    "vectorizer = pickle.load(open(\"C:\\\\Users\\\\hp\\\\Desktop\\\\college-5-2\\\\IR\\\\ir_project\\\\tfidf_output\\\\model.pickle\", \"rb\"))\n",
    "X = pickle.load(open(\"C:\\\\Users\\\\hp\\\\Desktop\\\\college-5-2\\\\IR\\\\ir_project\\\\tfidf_output\\\\tfidf_docs.pickle\", \"rb\"))\n",
    "\n",
    "# Ensure the corpus and TF-IDF matrix are aligned\n",
    "assert len(corpus) == X.shape[0], \"Corpus and TF-IDF matrix size mismatch\"\n",
    "\n",
    "# Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=12)\n",
    "lda.fit(X)\n",
    "\n",
    "# Transform documents to topic distribution\n",
    "doc_topic_dist = lda.transform(X)\n",
    "\n",
    "# Create an index\n",
    "index = {}\n",
    "for i, doc in enumerate(corpus):\n",
    "    index[doc] = doc_topic_dist[i]\n",
    "\n",
    "# Example query\n",
    "\n",
    "# # Calculate similarity between query and documents\n",
    "# similarity = cosine_similarity(query_topic_dist, doc_topic_dist)\n",
    "\n",
    "# # Rank documents\n",
    "# ranked_docs = similarity.argsort()[0][::-1]\n",
    "\n",
    "# # Display results\n",
    "# for idx in ranked_docs:\n",
    "#     print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixSimilarity<500 docs, 3 features>\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = vectorizer.transform(['give cat pill'])\n",
    "query_topic_dist = lda.transform(query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152839, 263354, 226768, 241838, 44022, 42149, 259511, 252432, 150214, 123637]\n"
     ]
    }
   ],
   "source": [
    "model_output= []\n",
    "cos=cosine_similarity(query_topic_dist,doc_topic_dist).flatten()\n",
    "map=dict()\n",
    "for x in range(cos.size):\n",
    "    map[cos[x]]=x\n",
    "\n",
    "myKey = list(map.keys())\n",
    "myKey.sort(reverse=True)\n",
    "for i in range(10):\n",
    "    model_output.append(map.get(myKey[i]))\n",
    "    \n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs for Topic 0 :\n",
      "[7, 11, 13, 15, 16, 17, 22, 25, 27, 29, 34, 40, 42, 44, 45, 48, 51, 55, 66, 74, 81, 82, 92, 97, 99, 100, 104, 109, 115, 116, 117, 124, 127, 129, 130, 137, 148, 151, 156, 157, 159, 161, 163, 169, 170, 171, 186, 191, 193, 195, 200, 223, 225, 231, 243, 247, 253, 254, 264, 265, 268, 269, 274, 293, 300, 313, 314, 317, 318, 320, 327, 337, 338, 340, 343, 348, 349, 350, 352, 353, 354, 355, 356, 357, 360, 371, 373, 376, 377, 379, 380, 385, 386, 398, 399, 404, 405, 408, 411, 413, 415, 425, 427, 428, 430, 439, 454, 468, 473, 479, 482, 486, 489, 491, 493, 495, 496, 498, 499]\n",
      "Similarity Scores:\n",
      "Topic 0 : 0.0\n",
      "Topic 1 : 0.0\n",
      "Topic 2 : 0.0\n",
      "Topic 3 : 0.0\n",
      "Topic 4 : 0.0\n",
      "The query is most similar to Topic 0 with a similarity score of 0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Topic ID to retrieve document IDs for\n",
    "topic_id = 0  # Replace with the desired topic ID\n",
    "\n",
    "# Retrieve document IDs for the given topic\n",
    "topic_docs = [doc_id for doc_id, doc_topic in enumerate(lda_model[corpus]) if max(doc_topic, key=lambda item: item[1])[0] == topic_id]\n",
    "\n",
    "# Print the document IDs for the topic\n",
    "print(\"Document IDs for Topic\", topic_id, \":\")\n",
    "print(topic_docs)\n",
    "\n",
    "# Preprocess the query\n",
    "query_tokens = [\"give away light cyclist light\"]\n",
    "\n",
    "# Convert the query to the binary vector representation\n",
    "query_vector = [1 if word in query_tokens else 0 for word in dictionary.token2id]\n",
    "\n",
    "# Calculate the Jaccard similarity between the query and each topic\n",
    "similarity_scores = []\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    topic_vector = lda_model.get_topic_terms(topic_id, topn=len(dictionary))\n",
    "    topic_terms = set([dictionary[id] for id, _ in topic_vector])\n",
    "    topic_vector = [1 if word in topic_terms else 0 for word in dictionary.token2id]\n",
    "    similarity_score = jaccard_score(query_vector, topic_vector)\n",
    "    similarity_scores.append(similarity_score)\n",
    "\n",
    "# Sort the topics by similarity score (descending order)\n",
    "sorted_topics = sorted(enumerate(similarity_scores), key=lambda item: -item[1])\n",
    "\n",
    "# Print the similarity scores\n",
    "print(\"Similarity Scores:\")\n",
    "for topic_id, similarity_score in sorted_topics:\n",
    "    print(\"Topic\", topic_id, \":\", similarity_score)\n",
    "\n",
    "# Output the most similar topic for the query\n",
    "most_similar_topic = sorted_topics[0][0]\n",
    "print(f\"The query is most similar to Topic {most_similar_topic} with a similarity score of {sorted_topics[0][1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
